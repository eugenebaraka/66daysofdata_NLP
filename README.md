# 66daysofdata_NLP

## Resources

### Books
- [Natural Language Processing with Python](https://www.nltk.org/book/)



# Day 1 of 66daysofdata

On my journey of #66daysofdata, I decided to start with the basics of Natural Language Processing (NLP). Today, I started with familiarizing myself with the NLTK library as well as some of the fundamental vocabularies (tokens, types, and lexical richness). **Tokens** are the number of characters in a text. **Types** are distinct and unique tokens in a text while **lexical richness** is the percentage of distinct tokens in a text. See you again tomorrow!

Book:
[Chapter 1. Natural Language Processing with Python](https://www.nltk.org/book/)

![lexical richness](./images/day1.PNG)

# Day 2 of 66daysofdata

Today, I touched on the overview of NLP usecase, including speech recognition, machine translation, and textual entailment. I also learned that NLP still has a lot of limitations such as performing common-sense reasoning or drawing on world knowledge to perform tasks. Here's is my image of the day. It explains the spoken dialog system pipeline. Now I have a high level understanding of how my Google Assistant works! Can't wait for tomorrow!

Book:
[Chapter 1. Natural Language Processing with Python](https://www.nltk.org/book/)

![spoken dialog system](./images/day2.PNG)

# Day 3 of 66daysofdata

I continued my journey by learning about WordNet and semantic similarity. WordNet is a lexical database of English words and their meaning. This is very useful in finding how close/similar two words are to each other by calculating their semantic similarity, which takes values 0 to 1, and the more similar are the closer to 1 their semantic similarity will be. Below is the calculated semantic similarity between the words "car" and "vehicle". Excited to learn more about how this concept is applied to a real-world NLP task. See you tomorrow!

Book:
[Chapter 2. Natural Language Processing with Python](https://www.nltk.org/book/)

![semantic similarity](./images/day3.png)

# Day 4 of 66daysofdata

Tokenization! This is a very important text pre-processing step in any NLP task. Tokenization is the process of breaking a text into a list of tokens (words and punctuations). I also learned how to parse HTML texts using BeautifulSoup. I practiced today's lesson by extracting the text from my latest blog post, parsing the HTML, extracting the text, and tokenizing it. See image below for the result.

Book:
[Chapter 3. Natural Language Processing with Python](https://www.nltk.org/book/)

![tokenization](./images/day4.PNG)

# Day 5 of 66daysofdata

My next lesson was about data normalization, which includes stemming and lemmatization. Stemming is a process of reducing a word to its root by removing the affixes, while lemmatization reduces the word to its base form but making sure the resulting word has a meaning. I am yet to be acquainted with this process, so I will have to practive a bit more in the coming days. I also deviated from NLP a bit and practiced web scrapping using Requests library. 

![normalization](./images/day5.png)