{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization (Revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PORTER STEMMING: ['In', 'earlier', 'program', 'exampl', 'we', 'have', 'often', 'convert', 'text', 'to', 'lowercas', 'befor', 'do', 'anyth', 'with', 'it', 'word', ',', 'e.g', '.', 'set', '(', 'w.lower', '(', ')', 'for', 'w', 'in', 'text', ')', '.', 'By', 'use', 'lower', '(', ')', ',', 'we', 'have', 'normal', 'the', 'text', 'to', 'lowercas', 'so', 'that', 'the', 'distinct', 'between', 'the', 'and', 'the', 'is', 'ignor', '.']\n",
      "LANCASTER STEMMING: ['in', 'ear', 'program', 'exampl', 'we', 'hav', 'oft', 'convert', 'text', 'to', 'lowercas', 'bef', 'doing', 'anyth', 'with', 'it', 'word', ',', 'e.g', '.', 'set', '(', 'w.lower', '(', ')', 'for', 'w', 'in', 'text', ')', '.', 'by', 'us', 'low', '(', ')', ',', 'we', 'hav', 'norm', 'the', 'text', 'to', 'lowercas', 'so', 'that', 'the', 'distinct', 'between', 'the', 'and', 'the', 'is', 'ign', '.']\n",
      "LEMMATIZATION: ['In', 'earlier', 'program', 'example', 'we', 'have', 'often', 'converted', 'text', 'to', 'lowercase', 'before', 'doing', 'anything', 'with', 'it', 'word', ',', 'e.g', '.', 'set', '(', 'w.lower', '(', ')', 'for', 'w', 'in', 'text', ')', '.', 'By', 'using', 'lower', '(', ')', ',', 'we', 'have', 'normalized', 'the', 'text', 'to', 'lowercase', 'so', 'that', 'the', 'distinction', 'between', 'The', 'and', 'the', 'is', 'ignored', '.']\n"
     ]
    }
   ],
   "source": [
    "raw = \"\"\"In earlier program examples we have often converted text to lowercase before doing anything with its words, e.g. set(w.lower() for w in text). \n",
    "By using lower(), we have normalized the text to lowercase so that the distinction between The and the is ignored.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "## Different Kinds of Stemming (Porter, Lancaster) and Lemmatization\n",
    "\n",
    "porter = nltk.PorterStemmer() #initializing porter stemmer\n",
    "lancaster = nltk.LancasterStemmer() #initializing lancaster stemmer\n",
    "wnl = nltk.WordNetLemmatizer() #initializing a lemmatizer\n",
    "\n",
    "from_porter = [porter.stem(t) for t in tokens]\n",
    "print(f\"PORTER STEMMING: {from_porter}\")\n",
    "from_lancaster = [lancaster.stem(t) for t in tokens]\n",
    "print(f\"LANCASTER STEMMING: {from_lancaster}\")\n",
    "from_lemmatization = [wnl.lemmatize(t) for t in tokens]\n",
    "print(\"LEMMATIZATION:\", from_lemmatization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both Porter and Lancaster stemmers fall short in giving tokens with real meaning. For example, the word \"example\" is stemmed as \"exampl\" and \"exampl\" is not a valid word. Therefore, we need to use Lemmatization to make sure our resulting tokens are valid dictionary words (lemmas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXCELLENTTTTTTTTTTTTTTTTTTT!!!!!!!!! lemmatization is AMAZING!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Use Regex to tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW TOKENS: ['In', 'earlier', 'program', 'examples', 'we', 'have', 'often', 'converted', 'text', 'to', 'lowercase', 'before', 'doing', 'anything', 'with', 'its', 'words,', 'e.g.', 'set(w.lower()', 'for', 'w', 'in', 'text).', '\\nBy', 'using', 'lower(),', 'we', 'have', 'normalized', 'the', 'text', 'to', 'lowercase', 'so', 'that', 'the', 'distinction', 'between', 'The', 'and', 'the', 'is', 'ignored.', 'Often', 'we', 'want', 'to', 'go', 'further', 'than', 'this,', '\\nand', 'strip', 'off', 'any', 'affixes,', 'a', 'task', 'known', 'as', 'stemming.', 'A', 'further', 'step', 'is', 'to', 'make', 'sure', 'that', 'the', 'resulting', 'form', 'is', 'a', 'known', 'word', 'in', 'a', 'dictionary,', 'a', 'task', 'known', '\\nas', 'lemmatization.', 'We', 'discuss', 'each', 'of', 'these', 'in', 'turn.']\n",
      "OLD TOKENS: ['In', 'earlier', 'program', 'examples', 'we', 'have', 'often', 'converted', 'text', 'to', 'lowercase', 'before', 'doing', 'anything', 'with', 'its', 'words', ',', 'e.g', '.', 'set', '(', 'w.lower', '(', ')', 'for', 'w', 'in', 'text', ')', '.', 'By', 'using', 'lower', '(', ')', ',', 'we', 'have', 'normalized', 'the', 'text', 'to', 'lowercase', 'so', 'that', 'the', 'distinction', 'between', 'The', 'and', 'the', 'is', 'ignored', '.', 'Often', 'we', 'want', 'to', 'go', 'further', 'than', 'this', ',', 'and', 'strip', 'off', 'any', 'affixes', ',', 'a', 'task', 'known', 'as', 'stemming', '.', 'A', 'further', 'step', 'is', 'to', 'make', 'sure', 'that', 'the', 'resulting', 'form', 'is', 'a', 'known', 'word', 'in', 'a', 'dictionary', ',', 'a', 'task', 'known', 'as', 'lemmatization', '.', 'We', 'discuss', 'each', 'of', 'these', 'in', 'turn', '.']\n"
     ]
    }
   ],
   "source": [
    "## Splitting words on white space\n",
    "\n",
    "regex_tokens = re.split(\" \", raw) \n",
    "print(\"NEW TOKENS:\", regex_tokens)\n",
    "print(\"OLD TOKENS:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by doing this both tokens are closely similar but in the NEW TOKENS, where there is a period or a comma, they go with the preceding token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'earlier', 'program', 'examples', 'we', 'have', 'often', 'converted', 'text', 'to', 'lowercase', 'before', 'doing', 'anything', 'with', 'its', 'words', 'e', 'g', 'set', 'w', 'lower', 'for', 'w', 'in', 'text', 'By', 'using', 'lower', 'we', 'have', 'normalized', 'the', 'text', 'to', 'lowercase', 'so', 'that', 'the', 'distinction', 'between', 'The', 'and', 'the', 'is', 'ignored', 'Often', 'we', 'want', 'to', 'go', 'further', 'than', 'this', 'and', 'strip', 'off', 'any', 'affixes', 'a', 'task', 'known', 'as', 'stemming', 'A', 'further', 'step', 'is', 'to', 'make', 'sure', 'that', 'the', 'resulting', 'form', 'is', 'a', 'known', 'word', 'in', 'a', 'dictionary', 'a', 'task', 'known', 'as', 'lemmatization', 'We', 'discuss', 'each', 'of', 'these', 'in', 'turn', '']\n"
     ]
    }
   ],
   "source": [
    "## Splitting on anything other than letters, digits, or underscore\n",
    "other_tokens = re.split(r\"\\W+\", raw)\n",
    "print(other_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"xx\".split('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ', ', '.', '. ', '(', '.', '() ', ' ', ' ', ' ', '). \\n', ' ', ' ', '(), ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '. ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ', \\n', ' ', ' ', ' ', ' ', ', ', ' ', ' ', ' ', ' ', '. ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ', ', ' ', ' ', ' \\n', ' ', '. ', ' ', ' ', ' ', ' ', ' ', ' ', '.']\n"
     ]
    }
   ],
   "source": [
    "print(re.split(r\"\\w+\", raw)) #this splits on words, digits, or underscores, which will give us punctuations and white spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'earlier', 'program', 'examples', 'we', 'have', 'often', 'converted', 'text', 'to', 'lowercase', 'before', 'doing', 'anything', 'with', 'its', 'words', ',', 'e', '.g', '.', 'set', '(w', '.lower', '(', ')', 'for', 'w', 'in', 'text', ')', '.', 'By', 'using', 'lower', '(', ')', ',', 'we', 'have', 'normalized', 'the', 'text', 'to', 'lowercase', 'so', 'that', 'the', 'distinction', 'between', 'The', 'and', 'the', 'is', 'ignored', '.', 'Often', 'we', 'want', 'to', 'go', 'further', 'than', 'this', ',', 'and', 'strip', 'off', 'any', 'affixes', ',', 'a', 'task', 'known', 'as', 'stemming', '.', 'A', 'further', 'step', 'is', 'to', 'make', 'sure', 'that', 'the', 'resulting', 'form', 'is', 'a', 'known', 'word', 'in', 'a', 'dictionary', ',', 'a', 'task', 'known', 'as', 'lemmatization', '.', 'We', 'discuss', 'each', 'of', 'these', 'in', 'turn', '.']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r\"\\w+|\\S\\w*\", raw)) #finds all words, digits, or underscores first. Then finds all non-white spaces (S), then any number of words\n",
    "# (1) a sequence of word characters. (2) if no match, any non-white spaces. (3) then a sequence of word charactters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_info = \"\"\"The client's email is really_hot_summer@client.com and their phone number is (531)-100-0000 or it can also be written as 531-100-0000. \n",
    "            The client paid at least $12.4 and their zipcode is 11004\"\"\"\n",
    "\n",
    "### I WILL BE BACK TO THIS LATTER, I PRETTY MUCH WANT TO CLIENT'S EXTRACT EMAIL, PHONE NUMBER, AMOUNT PAID, AND ZIPCODE...THESE THINGS ARE FREAKING HARDDDDDDDDDDDDDDDD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In earlier program examples we have often converted text to lowercase before '\n",
      " 'doing anything with its words, e.g.',\n",
      " 'set(w.lower() for w in text).',\n",
      " 'By using lower(), we have normalized the text to lowercase so that the '\n",
      " 'distinction between The and the is ignored.',\n",
      " 'Often we want to go further than this, \\n'\n",
      " 'and strip off any affixes, a task known as stemming.',\n",
      " 'A further step is to make sure that the resulting form is a known word in a '\n",
      " 'dictionary, a task known \\n'\n",
      " 'as lemmatization.',\n",
      " 'We discuss each of these in turn.']\n"
     ]
    }
   ],
   "source": [
    "## Sentence Segmentation: breaking your text into different sentences\n",
    "\n",
    "sents = nltk.sent_tokenize(raw)\n",
    "pprint.pprint(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat : 3, dog : 4, snake : 1, "
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
    "\n",
    "for word in sorted(fdist):\n",
    "    print(word, \":\", fdist[word], end= \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist[\"dog\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of chapter exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ Define a string s = 'colorless'. Write a Python statement that changes this to \"colourless\" using only the slice and concatenation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colourless'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'colorless'\n",
    "s[:4] + \"u\" + s[4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-ality, un-do, pre-heat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dish\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "string1 = \"dishes\"\n",
    "print(string1[:-2])\n",
    "string2 = \"running\"\n",
    "print(string2[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-57b1ecfe7f66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstring1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#NOPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "string1[-8] #NOPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dishes'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ We can specify a \"step\" size for the slice. The following returns every second character within the slice: monty[6:11:2]. It also works in the reverse direction: monty[10:5:-2] Try these for yourself, then experiment with different step values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mny'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty = \"Robin Monty\"\n",
    "monty[6:11:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "even\n",
      "odd\n"
     ]
    }
   ],
   "source": [
    "## odd or even\n",
    "\n",
    "def odd_or_even(number):\n",
    "    \"\"\"Function to determine if a \n",
    "    number is odd or even\"\"\"\n",
    "    if number % 2 == 0:\n",
    "        num = \"eovdedn\"[::2]\n",
    "    else:\n",
    "        num = \"eovdedn\"[1::2]\n",
    "    return num\n",
    "\n",
    "print(odd_or_even(50000))\n",
    "print(odd_or_even(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ What happens if you ask the interpreter to evaluate monty[::-1]? Explain why this is a reasonable result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ytnoM niboR'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty[::-1] #reverses the string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use from urllib import request and then request.urlopen('http://nltk.org/').read().decode('utf8') to access the contents of the URL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\nNLTK :: Natural Language Toolkit\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNLTK\\n\\n\\n\\nDocumentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNLTK Docume'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "import feedparser\n",
    "\n",
    "def cleans_url_content(url):\n",
    "    \"\"\" Reads content from the link, removes HTML markup\"\"\"\n",
    "    raw = request.urlopen(url).read().decode('utf8')\n",
    "    parsed = BeautifulSoup(raw, \"html.parser\").get_text()\n",
    "\n",
    "    return parsed\n",
    "\n",
    "\n",
    "conts = cleans_url_content(url = 'http://nltk.org/')\n",
    "conts[:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
